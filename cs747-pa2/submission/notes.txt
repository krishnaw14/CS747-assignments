___POLICY EVALUATION___

I computed the closed form solution (in matrix form) of the value function by manipulating the bellman equation as follows:

For a given state s and policy \pi, Bellman Equation is as follows: V^\pi (s) = \sum_{s'} T(s, \pi(s), s') [R(s,\pi(s),s') + \gamma V(s')]

\implies V^\pi (s) = \sum_{s'} T(s, \pi(s), s') R(s,\pi(s),s') + \gamma \sum_{s'} T(s, \pi(s), s') V(s')

Now I denote the first term of the RHS by \alpha^{\pi} (s) (= \sum_{s'} T(s, \pi(s), s') R(s,\pi(s),s')). The second term can be seen as a linear combination of V(s') weighted by T(s, \pi(s), s'). 

So writing the above bellman equation for all states in a matrix format:

[V^\pi (s_1), V^\pi (s_2) ... V^\pi (s_n) ]^T = [\alpha^\pi (s_1), \alpha^\pi (s_2) ... \alpha^\pi (s_n) ]^T + \gamma T [V^\pi (s_1), V^\pi (s_2) ... V^\pi (s_n) ]^T 

where T is a nxn matrix with each row denoting the transition probabilities for state s for the given policy (Second element of the first row of T is the probability that the agent will end up at state 2 starting from state 1 and following the given policy).

Essentially we have: V = \alpha + \gamma TV

\implies V = (I-\gamma T)\alpha

THIS FORMULA HAS BEEN USED FOR POLICY EVALUATION IN HPI IMPLEMENTATION.

___MDP RECONSTRUCTION___

Please refer to mdp_reconstruction.py for the logic used to construct the MDP. The basic idea for how an action can be optimal for a state in short term but suboptimal in long term. The script mdp_reconstruction.py can be run independently ($ python3 mdp_reconstruction.py for generation of mdp). 

Note that for different values of gamma, I used linear programming to generate optimal policies to check if I am getting 3 distinct policies for the given gamma value ranges. 